{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nerfstudio To D-NeRF dataset\n",
    "\n",
    "Set Up:\n",
    "\n",
    "1. Installl Nerfstudio and activate environment\n",
    "2. Run this notebook with the nerfstudio conda environment\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Update configuration relative to desired parameters\n",
    "2. Run the method\n",
    "\n",
    "#### Notes on D-NeRF dataset:\n",
    "\n",
    "In the nerfstudio DNeRF parser, `camera_angle_x` parameter (found in the transforms files) has a $1/tanh(0.5*x)$ relationship with the focal length. While I did mess around with this in [Desmos](https://www.desmos.com/calculator/xw0lodoghb) and initially selected a high $x=4.$ value. It didn't play well with kplanes (tested with scene contraction and varying near and far plane positions). In the end $x=0.6$ worked best despite how counter intuitive this is... The parameter is important for good rendering but is not directly recoverable from the nerfstudio dataset (it might be I just haven't looked into it).\n",
    "\n",
    "Otherwise the `rotation` parameter found for each frame doesn't seem to have any impact on performance (at least in the tests I ran with K-Planes, **this maybe different for other models!!!**)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Configuration\n",
    "---\n",
    "\n",
    "Information:\n",
    "1. `nerfstudio_fp` is the path **to the folder** containing `transforms.json`\n",
    "2. `output_fp` is the path to the folder you wish to write `transforms_train.json`, `transforms_test.json` and `transforms_val.json`\n",
    "3. `downscale_images_fp` is the folder name inside of you nerfstudio folder containing the downscaled images if you would rather save these than the original images. As dnerf format doesn't consider downscaled images, this will allow you to use them instead.\n",
    "4. `method` declares the way we recover the time values:\n",
    "    - `'exhaustive'`    : match image to frame (**super slow**)\n",
    "    - `'linear'`    : assign time given image name-index (**fast**) (e.g. image w/name `frame_{i}.png` is going to be at i/n time where n is the number of images and `i` is the index; this is the image-naming format used by nerfstudio-colamp process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerfstudio_fp ='path/to/folder/containing/tranform/file'\n",
    "video_fp = 'path/to/video.mp4'\n",
    "output_fp = 'path/to/write/transform/to'\n",
    "\n",
    "downscale_images_fp = 'path/to/folder/containing/downscalled/images'\n",
    "\n",
    "method = 'exhaustive'\n",
    "\n",
    "# handler(nerfstudio_fp, output_fp, video_fp, downscale_images_fp, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'handler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/barry/Desktop/DataCollection/datacollection/nstudio2dnerf.ipynb Cell 4\u001b[0m in \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/barry/Desktop/DataCollection/datacollection/nstudio2dnerf.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m downscale_images_fp \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mimages/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/barry/Desktop/DataCollection/datacollection/nstudio2dnerf.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mexhaustive\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/barry/Desktop/DataCollection/datacollection/nstudio2dnerf.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m handler(nerfstudio_fp, output_fp, video_fp, downscale_images_fp, method, rotation\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, camera_angle_x\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'handler' is not defined"
     ]
    }
   ],
   "source": [
    "nerfstudio_fp ='data/boat_colmap/'\n",
    "video_fp = 'data/boat/boat.mp4'\n",
    "output_fp = 'data/boat_exhaustive/'\n",
    "\n",
    "downscale_images_fp = 'images/'\n",
    "\n",
    "method = 'exhaustive'\n",
    "\n",
    "handler(nerfstudio_fp, output_fp, video_fp, downscale_images_fp, method, rotation=0.0, camera_angle_x=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# View the code\n",
    "---\n",
    "\n",
    "Information:\n",
    "1. Import dependencies. *Make sure nerfstudio has been downloaded*\n",
    "2. View the functions\n",
    "3. Change the functions (optional)\n",
    "4. Run the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils_ import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive SSIM Time Search\n",
    "\n",
    "**Args:**\n",
    "- d_fp, v_fp, img_fp: Path, previously discussed\n",
    "- transfors_fp: Path, path to transforms file\n",
    "\n",
    "**Notes:**\n",
    "1. Exhaustive search matches each image (e.g. `frame_0000.png`, `frame_0001.png`, ... ) to each frame in a video. \n",
    "2. Each image is a frame with png compression so direct image to frame comparison isn't possible\n",
    "3. Instead we compare w/ SSIM.\n",
    "4. This means:\n",
    "\n",
    "    a. Overlapping frames (such as monocular stationary camera with negligible dynamic motion) will have the same SSIM score and so we will get several frames which match\n",
    "        \n",
    "    b. We select the earliest occuring frame match as the time of the png image\n",
    "    \n",
    "    c. We accept that this may not always be the case so we add a threshold, whereby we search for the earliest match where SSIM > 0.95 , when this is not the case max(SSIM) > 0.9 is selected.\n",
    "    \n",
    "    d. Theoretically, this shouldn't be an issue for NeRF as SSIM threshold is high so should be negligible during NeRF evaluation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exhaustive(d_fp, o_fp, v_fp, img_fp, transforms_fp, shuffle:bool=True):\n",
    "    assert not os.path.exists(o_fp), 'Folder already exists, delete folder to run'\n",
    "\n",
    "    os.makedirs(o_fp)\n",
    "    with open(transforms_fp) as fp:\n",
    "        contents = fp.read()\n",
    "    transforms = json.loads(contents)\n",
    "    img_frames = transforms['frames'] # Directly access frame data\n",
    "\n",
    "    # Initialise opencv video object\n",
    "    video = cv2.VideoCapture(str(v_fp))\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f'Total number of frames to process {total_frames} \\n Total number of images to process {len(img_frames)}')\n",
    "\n",
    "\n",
    "    video_frame_counter = 0\n",
    "    matches = []\n",
    "\n",
    "    # Sort img_frames by filepath name\n",
    "    filenum = []\n",
    "    for img in img_frames:\n",
    "        num = img['file_path'].split('/')[-1].split('.')[-2].split('_')[-1]\n",
    "        filenum.append(int(num))\n",
    "    img_frames = [img for _, img in sorted(zip(filenum, img_frames), reverse=True)]\n",
    "    \n",
    "    print('Running')\n",
    "    iterator = tqdm(enumerate(img_frames))\n",
    "    # loop through each image in our colmap dataset\n",
    "    for idx, img in iterator:\n",
    "        fp = d_fp / img['file_path']\n",
    "        image = cv2.imread(str(fp), cv2.IMREAD_GRAYSCALE) # load image greyscale\n",
    "\n",
    "        SSIM = {\n",
    "            'max': 0.,\n",
    "            'idxs':[]\n",
    "        }\n",
    "        for idx_video in range(video_frame_counter, total_frames):\n",
    "            # Fetch frame from video\n",
    "            video.set(cv2.CAP_PROP_FRAME_COUNT, idx_video)\n",
    "            ret, frame = video.read()\n",
    "            print(ret)\n",
    "            if ret: frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # load frame greyscale\n",
    "            else: break\n",
    "\n",
    "            # Get SSIM\n",
    "            ssim_res = ssim(image, frame)\n",
    "            print(idx_video, ssim_res)\n",
    "            # Process SSIM\n",
    "            if ssim_res > 0.93: # if we meet ideal match\n",
    "                SSIM['max'] = 1.\n",
    "                SSIM['idxs'] = [idx_video] \n",
    "                # video_frame_counter = idx_video + 1\n",
    "                break\n",
    "            elif ssim_res > SSIM['max']: # if we have a new max\n",
    "                SSIM['max'] = ssim_res\n",
    "                SSIM['idxs'] = [idx_video]\n",
    "            elif ssim_res == SSIM['max']: # if we have the same max\n",
    "                SSIM['idxs'].append(idx_video)\n",
    "        \n",
    "\n",
    "        if SSIM['max'] < 0.9:\n",
    "            print(f'Image {idx} has no match: consider lowering SSIM threshold')\n",
    "        else:\n",
    "            idx_video_ = min(SSIM['idxs'])\n",
    "            matches.append({\n",
    "                \"frame\" : float(idx_video_),\n",
    "                \"image\" : idx\n",
    "                })\n",
    "\n",
    "    # shuffle data\n",
    "    if shuffle == True:\n",
    "        random.shuffle(matches)\n",
    "\n",
    "    # train split\n",
    "    train_split = 0.9\n",
    "    train_idx = int(train_split * len(matches))\n",
    "    train_data = matches[:train_idx]\n",
    "    # test split (on remaining data)\n",
    "    test_split = 0.9\n",
    "    test_index = train_idx + int(test_split * (len(matches) - len(train_data)))\n",
    "    test_data =  matches[train_idx : test_index]\n",
    "    # val split\n",
    "    val_data = matches[test_index:]\n",
    "\n",
    "\n",
    "    # Construct transform files\n",
    "    local_properties = {\n",
    "        \"roation\": 0.0,\n",
    "    }\n",
    "    data = [train_data, test_data, val_data]\n",
    "    for idx, d in enumerate(data):\n",
    "        file_ = {\n",
    "            \"camera_angle_x\": 0.6, # Seems to work best on the toy datasets I used\n",
    "            \"frames\":[]\n",
    "        }\n",
    "        # file_path\n",
    "        if idx == 0: file_path = Path(o_fp) / 'transforms_train.json'\n",
    "        elif idx == 1: file_path = Path(o_fp) / 'transforms_test.json'\n",
    "        elif idx == 2: file_path = Path(o_fp) / 'transforms_val.json'\n",
    "        \n",
    "        for match_idx in d:\n",
    "            img_data = img_frames[match_idx['image']]\n",
    "            time = float(match_idx['frame'] / total_frames)\n",
    "            fname = img_data['file_path'].split('/')[-1]\n",
    "            \n",
    "            file_[\"frames\"].append({\n",
    "            \"file_path\":f'./train/{fname}',\n",
    "            \"rotation\": local_properties['rotation'],\n",
    "            \"time\":time,\n",
    "            \"transform_matrix\":img_data['transform_matrix']\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blind Linear Time Search\n",
    "\n",
    "**Args:**\n",
    "- Same as before\n",
    "\n",
    "**Notes:**\n",
    "- Order collected images by frame number and linearly assign time value between 0 and 1.\n",
    "- This will make the motion linear with colmap extraction so it may not be ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(d_fp, o_fp, v_fp, img_fp, transforms_fp, shuffle:bool=True, rotation:float=0.0, camera_angle_x:float=0.0):\n",
    "    assert not os.path.exists(o_fp), 'Folder already exists, delete folder to run'\n",
    "\n",
    "    os.makedirs(o_fp)\n",
    "    os.makedirs(o_fp+'/train')\n",
    "    os.makedirs(o_fp+'/test')\n",
    "    os.makedirs(o_fp+'/val')\n",
    "    with open(transforms_fp) as fp:\n",
    "        contents = fp.read()\n",
    "    transforms = json.loads(contents)\n",
    "    img_frames = transforms['frames'] # Directly access frame data\n",
    "    print(f'Total number of images to process {len(img_frames)}')\n",
    "\n",
    "    local_properties = {\n",
    "        \"rotation\":rotation,\n",
    "    }\n",
    "\n",
    "    frames = []\n",
    "    for idx, img in enumerate(img_frames):\n",
    "        fname = img['file_path'].split('/')[-1].split('_')[-1].split('.')[0]\n",
    "\n",
    "        frames.append({\n",
    "            \"file_path\":f'./train/frame_{fname}',\n",
    "            \"rotation\": local_properties['rotation'],\n",
    "            \"time\":int(fname)/len(img_frames),\n",
    "            \"transform_matrix\":img['transform_matrix']\n",
    "        })\n",
    "    \n",
    "    # shuffle data\n",
    "    if shuffle == True:\n",
    "        random.shuffle(frames)\n",
    "\n",
    "    # train split\n",
    "    train_split = 0.9\n",
    "    train_idx = int(train_split * len(frames))\n",
    "    train_data = frames[:train_idx]\n",
    "    # test split (on remaining data)\n",
    "    test_split = 0.9\n",
    "    test_index = train_idx + int(test_split * (len(frames) - len(train_data)))\n",
    "    test_data =  frames[train_idx : test_index]\n",
    "    # val split\n",
    "    val_data = frames[test_index:]\n",
    "\n",
    "    print(f' Training dataset: {len(train_data)} | Testing dataset: {len(test_data)}')\n",
    "    data = [train_data, test_data, val_data]\n",
    "    for idx, d in enumerate(data):\n",
    "        file_ = {\n",
    "            \"camera_angle_x\": camera_angle_x,\n",
    "            \"frames\":[]\n",
    "        }\n",
    "        # file_path\n",
    "        if idx == 0: file_path = Path(o_fp) / 'transforms_train.json'\n",
    "        elif idx == 1: file_path = Path(o_fp) / 'transforms_test.json'\n",
    "        elif idx == 2: file_path = Path(o_fp) / 'transforms_val.json'\n",
    "        \n",
    "        for frame in d:\n",
    "            file_['frames'].append(frame)\n",
    "\n",
    "            if idx == 0:\n",
    "                destination = Path(o_fp) / 'train' / (frame['file_path'].split('/')[-1] + '.png')\n",
    "                source = img_fp / (frame['file_path'].split('/')[-1] + '.png')\n",
    "                shutil.copyfile(source, destination)\n",
    "            elif idx == 1:\n",
    "                destination = Path(o_fp) / 'test' / (frame['file_path'].split('/')[-1] + '.png')\n",
    "                source = img_fp / (frame['file_path'].split('/')[-1] + '.png')\n",
    "                shutil.copyfile(source, destination)\n",
    "            elif idx == 2:\n",
    "                destination = Path(o_fp) / 'val' / (frame['file_path'].split('/')[-1] + '.png')\n",
    "                source = img_fp / (frame['file_path'].split('/')[-1] + '.png')\n",
    "                shutil.copyfile(source, destination)\n",
    "        \n",
    "        with open(file_path, 'w') as fp:\n",
    "            json.dump(file_, fp)   \n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handler for nerfstudio2dnerf\n",
    "\n",
    "**Args:**\n",
    "- `d_fp`, Path, path to `transforms.json` **folder**\n",
    "- `o_fp`, Path, path to output folder\n",
    "- `v_fp`, Path, path to video\n",
    "- `img_fp`, Path, path to image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(d_fp, o_fp, v_fp, img_fp, meth, rotation:float=0.0, camera_angle_x:float=0.0):\n",
    "    d_fp = Path(d_fp)\n",
    "    v_fp = Path(v_fp)\n",
    "    img_fp = d_fp / img_fp\n",
    "\n",
    "    transforms_fp = d_fp/'transforms.json'\n",
    "\n",
    "    # Sanity Checks\n",
    "    pathchecks([d_fp, v_fp, img_fp])\n",
    "    folderchecks([d_fp, img_fp])\n",
    "    \n",
    "    # meth = 'linear'\n",
    "    # Handle exhaustive method\n",
    "    if meth == 'exhaustive':\n",
    "        exhaustive(d_fp, o_fp, v_fp, img_fp,transforms_fp)    \n",
    "    elif meth == 'linear':\n",
    "        linear(d_fp, o_fp, v_fp, img_fp,transforms_fp, rotation=rotation, camera_angle_x=camera_angle_x)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
